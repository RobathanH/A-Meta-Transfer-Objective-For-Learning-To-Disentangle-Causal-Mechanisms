{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Multivariate Structure Models to Determine Node Causal Parents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from tqdm.notebook import trange as tnrange\n",
    "\n",
    "from cs330.data_generators.multivariate_categorical import MultiCategorical\n",
    "from cs330.models.causal_parent_multivariate_model import *\n",
    "from cs330.models.full_causal_graph_multivariate_model import *\n",
    "from cs330.models.augmented_binary_models import *\n",
    "from cs330.models.augmented_binary_model_trainer import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Constants\n",
    "N = 10\n",
    "M = 3\n",
    "inner_lr = 1e-3\n",
    "outer_lr = 1e-2\n",
    "transfer_episode_count = 200\n",
    "transfer_episode_gradient_steps = 20\n",
    "transfer_episode_batch_size = 500\n",
    "pretrain_episode_gradient_steps = 200\n",
    "pretrain_episode_batch_size = 1000\n",
    "\n",
    "hypothesis_sample_count = 20 # Number of hypotheses to sample for each structure update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Create a causal parent multivariate model for the target node and data generator (including graph),\n",
    "learn causal structure, and plot a graph of structure adaptation for each possible parent node.\n",
    "\n",
    "Returns training curve log (runs, transfer episodes, target node, parent node), where each element is a likelihood\n",
    "for parent node to cause target node.\n",
    "Also returns the final predicted graphs after each run\n",
    "'''\n",
    "def full_graph_structure_training(data_gen, num_runs=1, savename=\"\", model_type=CausalParentMultivariateModel):\n",
    "    # Create model\n",
    "    model = model_type(N, M, hypothesis_sample_count)\n",
    "    hypothesis_opt = torch.optim.SGD(model.hypothesis_parameters(), lr=inner_lr)\n",
    "    structure_opt = torch.optim.RMSprop(model.structure_parameters(), lr=outer_lr)\n",
    "\n",
    "    # Store structure training logs\n",
    "    structure_likelihood_logs = torch.zeros(num_runs, transfer_episode_count + 1, *model.structure_likelihoods().shape).detach()\n",
    "    # Store full-graph predictions at the end of each run\n",
    "    graph_predictions = torch.zeros(num_runs, M, M)\n",
    "\n",
    "    for i in tnrange(num_runs, leave=False):\n",
    "        # Sample a new distribution\n",
    "        data_gen.reset_all_distributions()\n",
    "\n",
    "        # Reset structure parameters\n",
    "        model.reset_structure_parameters()\n",
    "\n",
    "        # Log initial parent likelihoods\n",
    "        structure_likelihood_logs[i, 0] = model.structure_likelihoods()\n",
    "\n",
    "        # Pretrain model on base distribution\n",
    "        # (Assume hypothesis where all other nodes are causal parents)\n",
    "        pretrain_hypotheses = model.pretrain_hypotheses()\n",
    "        for hypothesis_iter in tnrange(len(pretrain_hypotheses), leave=False):\n",
    "            pretrain_hypothesis = pretrain_hypotheses[hypothesis_iter]\n",
    "            for grad_step in tnrange(pretrain_episode_gradient_steps, leave=False):\n",
    "                # Sample from pretrain distribution\n",
    "                pretrain_samples = data_gen.sample(pretrain_episode_batch_size)\n",
    "            \n",
    "                hypothesis_opt.zero_grad()\n",
    "                logL = model.hypothesis_log_likelihood(pretrain_hypothesis, pretrain_samples)\n",
    "                loss = -torch.sum(logL) # Sum over loss for each target node, to train all at once\n",
    "                loss.backward()\n",
    "                hypothesis_opt.step()\n",
    "            model.save_pretrained(pretrain_hypothesis)\n",
    "\n",
    "        # Iterate through transfer episodes, updating structure params on each\n",
    "        transfers = tnrange(transfer_episode_count, leave=False)\n",
    "        for transfer_episode in transfers:\n",
    "            # Sample a new transfer distribution (adjusting root node functions)\n",
    "            data_gen.reset_root_distributions()\n",
    "\n",
    "            # Iterate through sampled hypotheses\n",
    "            hypothesis_list = model.sample_hypotheses()\n",
    "            hypothesis_online_logL = torch.zeros(len(hypothesis_list), M).detach()\n",
    "            for hypothesis_iter in range(len(hypothesis_list)):\n",
    "                hypothesis = hypothesis_list[hypothesis_iter]\n",
    "\n",
    "                # Accumulate log likelihood over training for this hypothesis\n",
    "                online_logL = torch.zeros(M).detach()\n",
    "\n",
    "                # Reset node function to pretrained state\n",
    "                model.load_pretrained(hypothesis)\n",
    "\n",
    "                # Train this hypothesis model\n",
    "                for grad_step in range(transfer_episode_gradient_steps):\n",
    "                    # Sample from transfer distribution\n",
    "                    train_samples = data_gen.sample(transfer_episode_batch_size)\n",
    "\n",
    "                    hypothesis_opt.zero_grad()\n",
    "                    logL = model.hypothesis_log_likelihood(hypothesis, train_samples)\n",
    "                    loss = -torch.sum(logL) # Sum over all target node losses to train all node functions at once\n",
    "                    loss.backward()\n",
    "                    hypothesis_opt.step()\n",
    "\n",
    "                    # Accumulate online log likelihood for each target node with this hypothesis\n",
    "                    online_logL += logL.detach()\n",
    "\n",
    "                # Save results for this hypothesis\n",
    "                hypothesis_online_logL[hypothesis_iter, :] = online_logL\n",
    "\n",
    "            # Update structure parameters\n",
    "            structure_opt.zero_grad()\n",
    "            model.compute_structure_gradients(hypothesis_list, hypothesis_online_logL)\n",
    "            structure_opt.step()\n",
    "\n",
    "            # Log structure params\n",
    "            current_structure_params = model.structure_likelihoods()\n",
    "            structure_likelihood_logs[i, transfer_episode + 1] = current_structure_params\n",
    "\n",
    "            # Add debug info to transfer episode progress bar\n",
    "            transfers.set_postfix(structure=current_structure_params.tolist())\n",
    "\n",
    "        # Store final graph prediction for this run after meta-training loop is complete\n",
    "        graph_predictions[i] = model.predicted_graph()\n",
    "        \n",
    "\n",
    "    # Optional plotting of results\n",
    "    if savename:\n",
    "        # Plot Graph for visualization\n",
    "        #data_gen.graph.visualize()\n",
    "        #if savename:\n",
    "            #plt.savefig(f\"{savename}.graph.png\")\n",
    "\n",
    "        # Plot structure parameter training progress\n",
    "        def plot_one_structure_likelihood_index(ax, logs, name):\n",
    "            ax.title.set_text(name)\n",
    "            for run in range(num_runs):\n",
    "                ax.plot(logs[run, :].detach().numpy())\n",
    "\n",
    "            ax.tick_params(axis='both', which='major', labelsize=13)\n",
    "            ax.axhline(1, c='lightgray', ls='--')\n",
    "            ax.axhline(0, c='lightgray', ls='--')\n",
    "            ax.set_xlim([0, transfer_episode_count])\n",
    "            ax.set_xlabel('Number of episodes', fontsize=14)\n",
    "            ax.set_ylabel(\"Hypothesis Likelihood\", fontsize=14)\n",
    "\n",
    "        structure_param_shape = model.structure_likelihoods().shape\n",
    "        if len(structure_param_shape) == 2:\n",
    "            rows, cols = structure_param_shape\n",
    "            fig, axarr = plt.subplots(rows, cols, figsize=(4.5 * cols, 3 * rows), constrained_layout=True)\n",
    "            for i in range(rows):\n",
    "                for j in range(cols):\n",
    "                    plot_one_structure_likelihood_index(axarr[i][j], structure_likelihood_logs[:, :, i, j], model.structure_param_name(i, j))\n",
    "        elif len(structure_param_shape) == 1:\n",
    "            rows = structure_param_shape[0]\n",
    "            fig, axarr = plt.subplots(rows, 1, figsize=(4.5, 3 * rows), constrained_layout=True)\n",
    "            for i in range(rows):\n",
    "                plot_one_structure_likelihood_index(axarr[i], structure_likelihood_logs[:, :, i], model.structure_param_name(i))\n",
    "        elif len(structure_param_shape) == 3:\n",
    "            rows = np.prod(structure_param_shape[:-1])\n",
    "            cols = structure_param_shape[-1]\n",
    "            fig, axarr = plt.subplots(rows, cols, figsize=(4.5 * cols, 3 * rows), constrained_layout=True)\n",
    "            for row in range(rows):\n",
    "                for col in range(cols):\n",
    "                    i, j = np.unravel_index(row, structure_param_shape[:-1])\n",
    "                    channel = col\n",
    "                    plot_one_structure_likelihood_index(axarr[row][col], structure_likelihood_logs[:, :, i, j, channel], model.structure_param_name(i, j, channel))\n",
    "        else:\n",
    "            raise ValueError(f\"Cannot plot structure logs with structure param shape {structure_param_shape}\")\n",
    "\n",
    "        plt.suptitle(savename)\n",
    "        plt.savefig(f\"{savename}.training.png\", facecolor='white', transparent=False)\n",
    "        plt.show()\n",
    "\n",
    "    return structure_likelihood_logs, graph_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for testing pairwise adaptation of binary transfer objective to multivariate case\n",
    "def pairwise_structure_training(data_gen, num_runs=1, savename=\"\"):\n",
    "    # Save the list of hypotheses considered by the binary model\n",
    "    hypothesis_list = CauseOnlyBinaryStructureModel(N, M, 0, 1).hypotheses()\n",
    "\n",
    "    # Iterate through all node pairs and save resulting predicted structures\n",
    "    node_pairs = []\n",
    "    for A in range(M - 1):\n",
    "        for B in range(A + 1, M):\n",
    "            node_pairs.append((A, B))\n",
    "\n",
    "    structure_predictions = {} # {NodePair -> {Hypothesis -> float}}\n",
    "    structure_training_curves = {} # {NodePair -> {Hypothesis -> torch.FloatTensor(shape = (num_runs, transfer_episodes + 1))}}\n",
    "    for i in tnrange(len(node_pairs)):\n",
    "        A, B = node_pairs[i]\n",
    "\n",
    "        # Average structure results over multiple runs (same graph, same causal edge functions, newly sampled root functions)\n",
    "        node_pair_predictions = {hypothesis: 0.0 for hypothesis in hypothesis_list}\n",
    "        node_pair_training_curves = {hypothesis: torch.zeros(num_runs, transfer_episode_count + 1).detach() for hypothesis in hypothesis_list}\n",
    "        for run in tnrange(num_runs, leave=False):\n",
    "            data_gen.reset_all_distributions() # Keeps the graph, but changes function parametrizing effect nodes by cause nodes\n",
    "\n",
    "            model = CauseOnlyBinaryStructureModel(N, M, A, B)\n",
    "            model_handler = AugmentedBinaryModelTrainer(\n",
    "                data_gen, model,\n",
    "                inner_lr = inner_lr,\n",
    "                outer_lr = outer_lr,\n",
    "                transfer_episode_count = transfer_episode_count,\n",
    "                transfer_episode_gradient_steps = transfer_episode_gradient_steps,\n",
    "                transfer_episode_batch_size = transfer_episode_batch_size,\n",
    "                pretrain_episode_batch_size = pretrain_episode_batch_size\n",
    "            )\n",
    "\n",
    "            # Meta-learn structure\n",
    "            model_handler.train_structure()\n",
    "\n",
    "            # Save structure training curve\n",
    "            for hyp_ind, hypothesis in enumerate(hypothesis_list):\n",
    "                node_pair_training_curves[hypothesis][run, :] = model_handler.structure_likelihoods[:, hyp_ind]\n",
    "\n",
    "            # Save final learned structure likelihood values\n",
    "            for hypothesis, probability in zip(model.hypotheses(), model_handler.structure_likelihoods[-1].tolist()):\n",
    "                node_pair_predictions[hypothesis] += probability / num_runs\n",
    "\n",
    "        # Save resulting structure prediction\n",
    "        structure_predictions[(A, B)] = node_pair_predictions\n",
    "        structure_training_curves[(A, B)] = node_pair_training_curves\n",
    "\n",
    "    # Plot Training curves for each hypothesis and node pair\n",
    "    if savename:\n",
    "        fig, axarr = plt.subplots(M, M, figsize=(4.5 * M, 3 * M), constrained_layout=True)\n",
    "        fig.suptitle(savename)\n",
    "        for A, B in node_pairs:\n",
    "            # Graph A -> B\n",
    "            ax = axarr[B][A]\n",
    "            ax.title.set_text(f\"Node {A} -> Node {B}\")\n",
    "            for run in range(num_runs):\n",
    "                ax.plot(structure_training_curves[(A, B)][Hypothesis.FORWARD_CAUSE][run].numpy())\n",
    "            ax.tick_params(axis='both', which='major', labelsize=13)\n",
    "            ax.axhline(1, c='lightgray', ls='--')\n",
    "            ax.axhline(0, c='lightgray', ls='--')\n",
    "            ax.set_xlim([0, transfer_episode_count])\n",
    "            ax.set_xlabel('Number of episodes', fontsize=14)\n",
    "            ax.set_ylabel(\"Hypothesis Likelihood\", fontsize=14)\n",
    "\n",
    "            # Graph B -> A\n",
    "            ax = axarr[A][B]\n",
    "            ax.title.set_text(f\"Node {B} -> Node {A}\")\n",
    "            for run in range(num_runs):\n",
    "                ax.plot(structure_training_curves[(A, B)][Hypothesis.BACKWARD_CAUSE][run].numpy())\n",
    "            ax.tick_params(axis='both', which='major', labelsize=13)\n",
    "            ax.axhline(1, c='lightgray', ls='--')\n",
    "            ax.axhline(0, c='lightgray', ls='--')\n",
    "            ax.set_xlim([0, transfer_episode_count])\n",
    "            ax.set_xlabel('Number of episodes', fontsize=14)\n",
    "            ax.set_ylabel(\"Hypothesis Likelihood\", fontsize=14)\n",
    "\n",
    "        plt.savefig(f\"{savename}.training.png\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run specific test cases on each multivariate structure model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Causal Graphs to test\n",
    "# List of elements (node_count, name, graph)\n",
    "data_tests = [\n",
    "    (\n",
    "        \"2_node_independent\",\n",
    "        np.array([\n",
    "            [0, 0],\n",
    "            [0, 0]\n",
    "        ])\n",
    "    ),\n",
    "    (\n",
    "        \"2_node_dependent\",\n",
    "        np.array([\n",
    "            [0, 0],\n",
    "            [1, 0]\n",
    "        ])\n",
    "    ),\n",
    "    (\n",
    "        \"3_node_1_independent\",\n",
    "        np.array([\n",
    "            [0, 0, 0],\n",
    "            [1, 0, 0],\n",
    "            [0, 0, 0]\n",
    "        ])\n",
    "    ),\n",
    "    (\n",
    "        \"3_node_chain\",\n",
    "        np.array([\n",
    "            [0, 0, 0],\n",
    "            [1, 0, 0],\n",
    "            [0, 1, 0]\n",
    "        ])\n",
    "    ),\n",
    "    (\n",
    "        \"3_node_collider\",\n",
    "        np.array([\n",
    "            [0, 0, 0],\n",
    "            [0, 0, 0],\n",
    "            [1, 1, 0]\n",
    "        ])\n",
    "    ),\n",
    "    (\n",
    "        \"3_node_confounder\",\n",
    "        np.array([\n",
    "            [0, 0, 0],\n",
    "            [1, 0, 0],\n",
    "            [1, 0, 0]\n",
    "        ])\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models to test\n",
    "# List of elements (name, model_class)\n",
    "# pairwise_binary_objective is a special case, with a different testing function\n",
    "model_tests = [\n",
    "    (\"pairwise_binary_objective\", None),\n",
    "    (\"causal_parent_sampling\", CausalParentMultivariateModel),\n",
    "    (\"causal_parent_all_hypotheses\", AllHypotheses_CausalParentMultivariateModel),\n",
    "    (\"edge_factored_full_causal_graph_all_hypotheses\", AllHypotheses_IndependentEdgeFullCausalGraphMultivariateModel),\n",
    "    (\"full_causal_graph_all_hypotheses\", AllHypotheses_FullCausalGraphMultivariateModel)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Constants\n",
    "num_runs = 1\n",
    "N = 10\n",
    "inner_lr = 1e-3\n",
    "outer_lr = 1e-2\n",
    "transfer_episode_count = 5\n",
    "transfer_episode_gradient_steps = 20\n",
    "transfer_episode_batch_size = 500\n",
    "pretrain_episode_gradient_steps = 200\n",
    "pretrain_episode_batch_size = 1000\n",
    "hypothesis_sample_count = 20 # Number of hypotheses to sample for each structure update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through tests and models\n",
    "for data_name, data_graph in data_tests[::-1]:\n",
    "    if not os.path.exists(data_name):\n",
    "        os.mkdir(data_name)\n",
    "    for model_name, model_class in model_tests:\n",
    "        print(f\"Testing {model_name} on {data_name}\")\n",
    "\n",
    "        M = len(data_graph)\n",
    "        data_gen = MultiCategorical(N, M)\n",
    "        data_gen.graph.B = np.array(data_graph)\n",
    "\n",
    "        if model_name == \"pairwise_binary_objective\":\n",
    "            pairwise_structure_training(data_gen, num_runs=num_runs, savename=f\"{data_name}/{model_name}\")\n",
    "        else:\n",
    "            full_graph_structure_training(data_gen, num_runs=num_runs, savename=f\"{data_name}/{model_name}\", model_type=model_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot Structural Hamming Distance from true graph over range of graph sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Structural Hamming Distance is the number of graph edge changes which must be made to convert one graph into the other\n",
    "# We add the augmentation that edges pointing in the wrong direction are counted as two mistakes\n",
    "# We also scale the SHD by the number of edges predicted by the structural model (all M**2 edges minus the diagonals, which are fixed)\n",
    "def SHD(graphA, graphB):\n",
    "    M = graphA.shape[0]\n",
    "    return torch.sum(torch.abs(graphA - graphB)) / (M**2 - M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "eb7920e3093183b727ed8516936ebf91c6c9837feb2019045bbbfcf824723b33"
  },
  "kernelspec": {
   "display_name": "Python 3.6.13 64-bit ('cs330': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
